{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vihaan/anaconda3/envs/vidtf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k400 were not used when initializing TimesformerModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/vihaan/anaconda3/envs/vidtf/lib/python3.8/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ucf6.zip already exists. Skipping download.\n",
      "Trainable parameteres: 10048518\n",
      "Total parameteres: 121263366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Done) : 100%|██████████| 180/180 [01:48<00:00,  1.67 batch/s, loss=1.6964, val/f1=0.255, train/f1=0.200]\n",
      "Epoch 1 (Done) : 100%|██████████| 180/180 [01:51<00:00,  1.61 batch/s, loss=0.2707, val/f1=0.925, train/f1=0.900]\n",
      "Epoch 2 (Done) : 100%|██████████| 180/180 [01:55<00:00,  1.56 batch/s, loss=0.2008, val/f1=0.946, train/f1=0.996]\n",
      "Epoch 3 (Done) : 100%|██████████| 180/180 [01:52<00:00,  1.60 batch/s, loss=0.1911, val/f1=0.950, train/f1=0.996]\n",
      "Epoch 4 (Done) : 100%|██████████| 180/180 [01:47<00:00,  1.68 batch/s, loss=0.1768, val/f1=0.954, train/f1=0.998]\n",
      "Epoch 5 (Done) : 100%|██████████| 180/180 [01:50<00:00,  1.63 batch/s, loss=0.1663, val/f1=0.945, train/f1=0.998]\n",
      "Epoch 6 (Done) : 100%|██████████| 180/180 [01:52<00:00,  1.60 batch/s, loss=0.1906, val/f1=0.944, train/f1=0.998]\n",
      "Epoch 7 (Done) : 100%|██████████| 180/180 [01:52<00:00,  1.60 batch/s, loss=0.1912, val/f1=0.950, train/f1=1.000]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from video_transformers import VideoModel\n",
    "from video_transformers.backbones.transformers import TransformersBackbone\n",
    "from video_transformers.data import VideoDataModule\n",
    "from video_transformers.heads import LinearHead\n",
    "from video_transformers.trainer import trainer_factory\n",
    "from video_transformers.utils.file import download_ucf6\n",
    "\n",
    "backbone = TransformersBackbone(\"facebook/timesformer-base-finetuned-k400\", num_unfrozen_stages=1)\n",
    "\n",
    "download_ucf6(\"./\")\n",
    "datamodule = VideoDataModule(\n",
    "    train_root=\"ucf6/train\",\n",
    "    val_root=\"ucf6/val\",\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    num_timesteps=8,\n",
    "    preprocess_input_size=224,\n",
    "    preprocess_clip_duration=1,\n",
    "    preprocess_means=backbone.mean,\n",
    "    preprocess_stds=backbone.std,\n",
    "    preprocess_min_short_side=256,\n",
    "    preprocess_max_short_side=320,\n",
    "    preprocess_horizontal_flip_p=0.5,\n",
    ")\n",
    "\n",
    "head = LinearHead(hidden_size=backbone.num_features, num_classes=datamodule.num_classes)\n",
    "model = VideoModel(backbone, head)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "Trainer = trainer_factory(\"single_label_classification\")\n",
    "trainer = Trainer(datamodule, model, optimizer=optimizer, max_epochs=8)\n",
    "\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from video_transformers import VideoModel\n",
    "from video_transformers.backbones.transformers import TransformersBackbone\n",
    "from video_transformers.data import VideoDataModule\n",
    "from video_transformers.heads import MultiLabelLinearHead\n",
    "from video_transformers.trainer import trainer_factory\n",
    "from video_transformers.utils.file import download_ucf6\n",
    "\n",
    "backbone = TransformersBackbone(\"facebook/timesformer-base-finetuned-k400\", num_unfrozen_stages=1)\n",
    "\n",
    "download_ucf6(\"./\")\n",
    "datamodule = VideoDataModule(\n",
    "    train_root=\"ucf6/train\",\n",
    "    val_root=\"ucf6/val\",\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    num_timesteps=8,\n",
    "    preprocess_input_size=224,\n",
    "    preprocess_clip_duration=1,\n",
    "    preprocess_means=backbone.mean,\n",
    "    preprocess_stds=backbone.std,\n",
    "    preprocess_min_short_side=256,\n",
    "    preprocess_max_short_side=320,\n",
    "    preprocess_horizontal_flip_p=0.5,\n",
    ")\n",
    "\n",
    "# Assume MultiLabelLinearHead exists and is appropriate for multi-label tasks\n",
    "head = MultiLabelLinearHead(hidden_size=backbone.num_features, num_classes=datamodule.num_classes)\n",
    "model = VideoModel(backbone, head)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set up the loss function for multi-label classification\n",
    "loss_function = BCEWithLogitsLoss()\n",
    "\n",
    "# Create the trainer for multi-label classification\n",
    "Trainer = trainer_factory(\"multi_label_classification\")\n",
    "trainer = Trainer(datamodule, model, optimizer=optimizer, loss_function=loss_function, max_epochs=8)\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "import evaluate\n",
    "\n",
    "class Combine:\n",
    "    # place holder for evaluate.combine till https://github.com/huggingface/evaluate/issues/234 fixed\n",
    "    def __init__(self, metrics: List[str]):\n",
    "        self.metrics = [evaluate.load(metric) if isinstance(metric, str) else metric for metric in metrics]\n",
    "        print(self.metrics)\n",
    "\n",
    "    def add_batch(self, predictions: Any, references: Any):\n",
    "        for metric in self.metrics:\n",
    "            metric.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "    def compute(self, **kwargs):\n",
    "        results = {}\n",
    "        zero_division = kwargs.get(\"zero_division\", \"warn\")\n",
    "        kwargs.pop(\"zero_division\")\n",
    "        for metric in self.metrics:\n",
    "            if metric.name == \"precision\":\n",
    "                results.update(metric.compute(zero_division=zero_division, **kwargs))\n",
    "            else:\n",
    "                results.update(metric.compute(**kwargs))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvaluationModule(name: \"f1\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'references': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}, usage: \"\"\"\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n",
      "    pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n",
      "    average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n",
      "\n",
      "        - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n",
      "        - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
      "        - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
      "        - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\n",
      "        - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    f1 (`float` or `array` of `float`): F1 score or list of f1 scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher f1 scores are better.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple binary example\n",
      "        >>> f1_metric = evaluate.load(\"f1\")\n",
      "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'f1': 0.5}\n",
      "\n",
      "    Example 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\n",
      "        >>> f1_metric = evaluate.load(\"f1\")\n",
      "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\n",
      "        >>> print(round(results['f1'], 2))\n",
      "        0.67\n",
      "\n",
      "    Example 3-The same simple binary example as in Example 1, but with `sample_weight` included.\n",
      "        >>> f1_metric = evaluate.load(\"f1\")\n",
      "        >>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\n",
      "        >>> print(round(results['f1'], 2))\n",
      "        0.35\n",
      "\n",
      "    Example 4-A multiclass example, with different values for the `average` input.\n",
      "        >>> predictions = [0, 2, 1, 0, 0, 1]\n",
      "        >>> references = [0, 1, 2, 0, 1, 2]\n",
      "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\n",
      "        >>> print(round(results['f1'], 2))\n",
      "        0.27\n",
      "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\n",
      "        >>> print(round(results['f1'], 2))\n",
      "        0.33\n",
      "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n",
      "        >>> print(round(results['f1'], 2))\n",
      "        0.27\n",
      "        >>> results = f1_metric.compute(predictions=predictions, references=references, average=None)\n",
      "        >>> print(results)\n",
      "        {'f1': array([0.8, 0. , 0. ])}\n",
      "\n",
      "    Example 5-A multi-label example\n",
      "        >>> f1_metric = evaluate.load(\"f1\", \"multilabel\")\n",
      "        >>> results = f1_metric.compute(predictions=[[0, 1, 1], [1, 1, 0]], references=[[0, 1, 1], [0, 1, 0]], average=\"macro\")\n",
      "        >>> print(round(results['f1'], 2))\n",
      "        0.67\n",
      "\"\"\", stored examples: 0), EvaluationModule(name: \"precision\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted class labels.\n",
      "    references (`list` of `int`): Actual class labels.\n",
      "    labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`. If `average` is `None`, it should be the label order. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n",
      "    pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n",
      "    average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n",
      "\n",
      "        - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n",
      "        - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
      "        - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
      "        - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\n",
      "        - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "    zero_division (`int` or `string`): Sets the value to return when there is a zero division. Defaults to 'warn'.\n",
      "\n",
      "        - 0: Returns 0 when there is a zero division.\n",
      "        - 1: Returns 1 when there is a zero division.\n",
      "        - 'warn': Raises warnings and then returns 0 when there is a zero division.\n",
      "\n",
      "Returns:\n",
      "    precision (`float` or `array` of `float`): Precision score or list of precision scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher values indicate that fewer negative examples were incorrectly labeled as positive, which means that, generally, higher scores are better.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple binary example\n",
      "        >>> precision_metric = evaluate.load(\"precision\")\n",
      "        >>> results = precision_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'precision': 0.5}\n",
      "\n",
      "    Example 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\n",
      "        >>> precision_metric = evaluate.load(\"precision\")\n",
      "        >>> results = precision_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\n",
      "        >>> print(round(results['precision'], 2))\n",
      "        0.67\n",
      "\n",
      "    Example 3-The same simple binary example as in Example 1, but with `sample_weight` included.\n",
      "        >>> precision_metric = evaluate.load(\"precision\")\n",
      "        >>> results = precision_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\n",
      "        >>> print(results)\n",
      "        {'precision': 0.23529411764705882}\n",
      "\n",
      "    Example 4-A multiclass example, with different values for the `average` input.\n",
      "        >>> predictions = [0, 2, 1, 0, 0, 1]\n",
      "        >>> references = [0, 1, 2, 0, 1, 2]\n",
      "        >>> results = precision_metric.compute(predictions=predictions, references=references, average='macro')\n",
      "        >>> print(results)\n",
      "        {'precision': 0.2222222222222222}\n",
      "        >>> results = precision_metric.compute(predictions=predictions, references=references, average='micro')\n",
      "        >>> print(results)\n",
      "        {'precision': 0.3333333333333333}\n",
      "        >>> results = precision_metric.compute(predictions=predictions, references=references, average='weighted')\n",
      "        >>> print(results)\n",
      "        {'precision': 0.2222222222222222}\n",
      "        >>> results = precision_metric.compute(predictions=predictions, references=references, average=None)\n",
      "        >>> print([round(res, 2) for res in results['precision']])\n",
      "        [0.67, 0.0, 0.0]\n",
      "\"\"\", stored examples: 0), EvaluationModule(name: \"recall\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
      "Args:\n",
      "- **predictions** (`list` of `int`): The predicted labels.\n",
      "- **references** (`list` of `int`): The ground truth labels.\n",
      "- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `binary`, and their order when average is `None`. Labels present in the data can be excluded in this input, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Defaults to None.\n",
      "- **pos_label** (`int`): The class label to use as the 'positive class' when calculating the recall. Defaults to `1`.\n",
      "- **average** (`string`): This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n",
      "    - `'binary'`: Only report results for the class specified by `pos_label`. This is applicable only if the target labels and predictions are binary.\n",
      "    - `'micro'`: Calculate metrics globally by counting the total true positives, false negatives, and false positives.\n",
      "    - `'macro'`: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
      "    - `'weighted'`: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. Note that it can result in an F-score that is not between precision and recall.\n",
      "    - `'samples'`: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n",
      "- **sample_weight** (`list` of `float`): Sample weights Defaults to `None`.\n",
      "- **zero_division** (): Sets the value to return when there is a zero division. Defaults to .\n",
      "    - `'warn'`: If there is a zero division, the return value is `0`, but warnings are also raised.\n",
      "    - `0`: If there is a zero division, the return value is `0`.\n",
      "    - `1`: If there is a zero division, the return value is `1`.\n",
      "\n",
      "Returns:\n",
      "- **recall** (`float`, or `array` of `float`): Either the general recall score, or the recall scores for individual classes, depending on the values input to `labels` and `average`. Minimum possible value is 0. Maximum possible value is 1. A higher recall means that more of the positive examples have been labeled correctly. Therefore, a higher recall is generally considered better.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example with some errors\n",
      "        >>> recall_metric = evaluate.load('recall')\n",
      "        >>> results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1])\n",
      "        >>> print(results)\n",
      "        {'recall': 0.6666666666666666}\n",
      "\n",
      "    Example 2-The same example as Example 1, but with `pos_label=0` instead of the default `pos_label=1`.\n",
      "        >>> recall_metric = evaluate.load('recall')\n",
      "        >>> results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1], pos_label=0)\n",
      "        >>> print(results)\n",
      "        {'recall': 0.5}\n",
      "\n",
      "    Example 3-The same example as Example 1, but with `sample_weight` included.\n",
      "        >>> recall_metric = evaluate.load('recall')\n",
      "        >>> sample_weight = [0.9, 0.2, 0.9, 0.3, 0.8]\n",
      "        >>> results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1], sample_weight=sample_weight)\n",
      "        >>> print(results)\n",
      "        {'recall': 0.55}\n",
      "\n",
      "    Example 4-A multiclass example, using different averages.\n",
      "        >>> recall_metric = evaluate.load('recall')\n",
      "        >>> predictions = [0, 2, 1, 0, 0, 1]\n",
      "        >>> references = [0, 1, 2, 0, 1, 2]\n",
      "        >>> results = recall_metric.compute(predictions=predictions, references=references, average='macro')\n",
      "        >>> print(results)\n",
      "        {'recall': 0.3333333333333333}\n",
      "        >>> results = recall_metric.compute(predictions=predictions, references=references, average='micro')\n",
      "        >>> print(results)\n",
      "        {'recall': 0.3333333333333333}\n",
      "        >>> results = recall_metric.compute(predictions=predictions, references=references, average='weighted')\n",
      "        >>> print(results)\n",
      "        {'recall': 0.3333333333333333}\n",
      "        >>> results = recall_metric.compute(predictions=predictions, references=references, average=None)\n",
      "        >>> print(results)\n",
      "        {'recall': array([1., 0., 0.])}\n",
      "\"\"\", stored examples: 0)]\n",
      "<__main__.Combine object at 0x7558c38ffeb0>\n"
     ]
    }
   ],
   "source": [
    "loaded_metrics = Combine([evaluate.load(\"f1\", \"multilabel\"), \"precision\", \"recall\"])\n",
    "print(loaded_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `evaluate.load()` not found.\n"
     ]
    }
   ],
   "source": [
    "?evaluate.load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
